{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n",
    "\n",
    "data_sources = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input - S3 Source: raw-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources.append(ProcessingInput(\n",
    "    source=\"s3://ml-dataset-raw-s3/raw-out/\", # You can override this to point to other dataset on S3\n",
    "    destination=\"/opt/ml/processing/raw-out\",\n",
    "    input_name=\"raw-out\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output: S3 settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Wrangler export storage bucket: sagemaker-us-east-1-456441140195\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import uuid\n",
    "import sagemaker\n",
    "\n",
    "# Sagemaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# You can configure this with your own bucket name, e.g.\n",
    "# bucket = \"my-bucket\"\n",
    "bucket = sess.default_bucket()\n",
    "print(f\"Data Wrangler export storage bucket: {bucket}\")\n",
    "\n",
    "# unique flow export ID\n",
    "flow_export_id = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "flow_export_name = f\"flow-{flow_export_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow S3 export result path: s3://sagemaker-us-east-1-456441140195/export-flow-13-21-38-01-5829c9ff/output\n"
     ]
    }
   ],
   "source": [
    "# Output name is auto-generated from the select node's ID + output name from the flow file.\n",
    "output_name = \"bc3fa374-27dc-4e5b-a1a1-58ea5addeb1b.default\"\n",
    "\n",
    "s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "s3_output_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "print(f\"Flow S3 export result path: {s3_output_path}\")\n",
    "\n",
    "processing_job_output = ProcessingOutput(\n",
    "    output_name=output_name,\n",
    "    source=\"/opt/ml/processing/output\",\n",
    "    destination=s3_output_path,\n",
    "    s3_upload_mode=\"EndOfJob\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Flow to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flow file from current notebook working directory: /root\n",
      "Data Wrangler flow ml-airport-delay.flow uploaded to s3://sagemaker-us-east-1-456441140195/data_wrangler_flows/flow-13-21-38-01-5829c9ff.flow\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# name of the flow file which should exist in the current notebook working directory\n",
    "flow_file_name = \"Ml-Airport-01-XGBoost-v2.flow\"\n",
    "\n",
    "# Load .flow file from current notebook working directory \n",
    "!echo \"Loading flow file from current notebook working directory: $PWD\"\n",
    "\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "\n",
    "# Upload flow to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name, bucket, f\"data_wrangler_flows/{flow_export_name}.flow\", ExtraArgs={\"ServerSideEncryption\": \"aws:kms\"})\n",
    "\n",
    "flow_s3_uri = f\"s3://{bucket}/data_wrangler_flows/{flow_export_name}.flow\"\n",
    "\n",
    "print(f\"Data Wrangler flow {flow_file_name} uploaded to {flow_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input - Flow: ml-airport-delay.flow\n",
    "flow_input = ProcessingInput(\n",
    "    source=flow_s3_uri,\n",
    "    destination=\"/opt/ml/processing/flow\",\n",
    "    input_name=\"flow\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Processing Job \n",
    "## Job Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAM role for executing the processing job.\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "# Unique processing job name. Give a unique name every time you re-execute processing jobs\n",
    "processing_job_name = f\"data-wrangler-flow-processing-{flow_export_id}\"\n",
    "\n",
    "# Data Wrangler Container URL.\n",
    "container_uri = \"663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.x\"\n",
    "# Pinned Data Wrangler Container URL. \n",
    "container_uri_pinned = \"663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.13.2\"\n",
    "\n",
    "# Processing Job Instance count and instance type.\n",
    "instance_count = 2\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "# Size in GB of the EBS volume to use for storing data during processing\n",
    "volume_size_in_gb = 30\n",
    "\n",
    "# Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# Network Isolation mode; default is off\n",
    "enable_network_isolation = False\n",
    "\n",
    "# List of tags to be passed to the processing job\n",
    "user_tags = []\n",
    "\n",
    "# Output configuration used as processing job container arguments \n",
    "output_config = {\n",
    "    output_name: {\n",
    "        \"content_type\": output_content_type\n",
    "    }\n",
    "}\n",
    "\n",
    "# KMS key for per object encryption; default is None\n",
    "kms_key = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  data-wrangler-flow-processing-13-21-38-01-5829c9ff\n",
      "Inputs:  [{'InputName': 'flow', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-456441140195/data_wrangler_flows/flow-13-21-38-01-5829c9ff.flow', 'LocalPath': '/opt/ml/processing/flow', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'raw-out', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://ml-dataset-raw-s3/raw-out/', 'LocalPath': '/opt/ml/processing/raw-out', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'bc3fa374-27dc-4e5b-a1a1-58ea5addeb1b.default', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-456441140195/export-flow-13-21-38-01-5829c9ff/output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import Processor\n",
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    volume_size_in_gb=volume_size_in_gb,\n",
    "    network_config=NetworkConfig(enable_network_isolation=enable_network_isolation),\n",
    "    sagemaker_session=sess,\n",
    "    output_kms_key=kms_key,\n",
    "    tags=user_tags\n",
    ")\n",
    "\n",
    "# Start Job\n",
    "processor.run(\n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[processing_job_output],\n",
    "    arguments=[f\"--output-config '{json.dumps(output_config)}'\"],\n",
    "    wait=False,\n",
    "    logs=False,\n",
    "    job_name=processing_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Status & S3 Output Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job results are saved to S3 path: s3://sagemaker-us-east-1-456441140195/export-flow-13-21-38-01-5829c9ff/output/data-wrangler-flow-processing-13-21-38-01-5829c9ff\n",
      "...................................................................!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'flow',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-456441140195/data_wrangler_flows/flow-13-21-38-01-5829c9ff.flow',\n",
       "    'LocalPath': '/opt/ml/processing/flow',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}},\n",
       "  {'InputName': 'raw-out',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://ml-dataset-raw-s3/raw-out/',\n",
       "    'LocalPath': '/opt/ml/processing/raw-out',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bc3fa374-27dc-4e5b-a1a1-58ea5addeb1b.default',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-456441140195/export-flow-13-21-38-01-5829c9ff/output',\n",
       "     'LocalPath': '/opt/ml/processing/output',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False}]},\n",
       " 'ProcessingJobName': 'data-wrangler-flow-processing-13-21-38-01-5829c9ff',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2,\n",
       "   'InstanceType': 'ml.m5.4xlarge',\n",
       "   'VolumeSizeInGB': 30}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       " 'AppSpecification': {'ImageUri': '663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.x',\n",
       "  'ContainerArguments': ['--output-config \\'{\"bc3fa374-27dc-4e5b-a1a1-58ea5addeb1b.default\": {\"content_type\": \"CSV\"}}\\'']},\n",
       " 'NetworkConfig': {'EnableInterContainerTrafficEncryption': False,\n",
       "  'EnableNetworkIsolation': False},\n",
       " 'RoleArn': 'arn:aws:iam::456441140195:role/LabRole',\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:456441140195:processing-job/data-wrangler-flow-processing-13-21-38-01-5829c9ff',\n",
       " 'ProcessingJobStatus': 'Completed',\n",
       " 'ProcessingEndTime': datetime.datetime(2022, 2, 13, 21, 45, 15, 150000, tzinfo=tzlocal()),\n",
       " 'ProcessingStartTime': datetime.datetime(2022, 2, 13, 21, 43, 56, 21000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2022, 2, 13, 21, 45, 15, 493000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2022, 2, 13, 21, 39, 26, 908000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '49b94221-a908-4b21-adc5-76b479fda653',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '49b94221-a908-4b21-adc5-76b479fda653',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1957',\n",
       "   'date': 'Sun, 13 Feb 2022 21:45:17 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_job_results_path = f\"s3://{bucket}/{s3_output_prefix}/{processing_job_name}\"\n",
    "print(f\"Job results are saved to S3 path: {s3_job_results_path}\")\n",
    "\n",
    "job_result = sess.wait_for_processing_job(processing_job_name)\n",
    "job_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_optional_steps = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Processed Data into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q awswrangler pandas\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABE</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>7.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABE</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>77.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABE</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>51.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABE</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>30.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABE</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>17.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>2018-09-23</td>\n",
       "      <td>7.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>2018-09-24</td>\n",
       "      <td>5.472222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>2018-09-25</td>\n",
       "      <td>3.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>2018-09-26</td>\n",
       "      <td>19.457143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>2018-09-27</td>\n",
       "      <td>3.986111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ORIGIN     FL_DATE  DEP_DELAY\n",
       "0      ABE  2018-01-01   7.833333\n",
       "1      ABE  2018-01-02  77.375000\n",
       "2      ABE  2018-01-03  51.333333\n",
       "3      ABE  2018-01-04  30.125000\n",
       "4      ABE  2018-01-05  17.375000\n",
       "..     ...         ...        ...\n",
       "995    ABQ  2018-09-23   7.714286\n",
       "996    ABQ  2018-09-24   5.472222\n",
       "997    ABQ  2018-09-25   3.647059\n",
       "998    ABQ  2018-09-26  19.457143\n",
       "999    ABQ  2018-09-27   3.986111\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunksize = 1000\n",
    "\n",
    "if output_content_type.upper() == \"CSV\":\n",
    "    dfs = wr.s3.read_csv(s3_output_path, chunksize=chunksize)\n",
    "elif output_content_type.upper() == \"PARQUET\":\n",
    "    dfs = wr.s3.read_parquet(s3_output_path, chunked=chunksize)\n",
    "else:\n",
    "    print(f\"Unexpected output content type {output_content_type}\") \n",
    "\n",
    "df = next(dfs)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n",
    "### Set Training Data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training input data path: s3://sagemaker-us-east-1-456441140195/export-flow-13-21-38-01-5829c9ff/output/data-wrangler-flow-processing-13-21-38-01-5829c9ff\n"
     ]
    }
   ],
   "source": [
    "s3_training_input_path = s3_job_results_path\n",
    "print(f\"training input data path: {s3_training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the algorithm and training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "hyperparameters = {\n",
    "    \"max_depth\":\"5\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"10\",\n",
    "}\n",
    "train_content_type = (\n",
    "    \"application/x-parquet\" if output_content_type.upper() == \"PARQUET\"\n",
    "    else \"text/csv\"\n",
    ")\n",
    "train_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=s3_training_input_path,\n",
    "    content_type=train_content_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-13 21:49:24 Starting - Starting the training job...\n",
      "2022-02-13 21:49:47 Starting - Launching requested ML instancesProfilerReport-1644788964: InProgress\n",
      "......\n",
      "2022-02-13 21:50:55 Starting - Preparing the instances for training......\n",
      "2022-02-13 21:51:54 Downloading - Downloading input data...\n",
      "2022-02-13 21:52:08 Training - Downloading the training image...\n",
      "2022-02-13 21:52:54 Uploading - Uploading generated training model.\u001b[34m[2022-02-13 21:52:51.324 ip-10-0-227-25.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 1062274 rows and 2 columns\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:0.35025\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:0.24458\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:0.17126\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:0.12002\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:0.08410\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:0.05887\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:0.04124\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:0.02878\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:0.02015\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:0.01412\u001b[0m\n",
      "\n",
      "2022-02-13 21:53:08 Completed - Training job completed\n",
      "Training seconds: 68\n",
      "Billable seconds: 68\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    iam_role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "estimator.fit({\"train\": train_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training job name: {estimator.latest_training_job.job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the EndPoint ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"airport-data-delay-{estimator.latest_training_job.job_name}\"\n",
    "\n",
    "estimator_inference = estimator.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\", endpoint_name = endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Endpoint name: {estimator_inference.endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas detalles<p>\n",
    "https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
